{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with PySpark Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use classification model to classify flower types by their characteristics using the famous [iris dataset]().\n",
    "\n",
    "This small dataset from 1936 is often used for testing out machine learning algorithms and visualizations (for example, Scatter Plot). Each row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimeters.\n",
    "\n",
    "We have already explored the dataset using [this notebook](https://www.kaggle.com/aceccon/1-iris-dataset-data-exploration/notebook).\n",
    "\n",
    "**Our goal is to get >90% of our predictions right (>90% accuracy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlcontext = SQLContext(sc)\n",
    "raw_df = sqlcontext.read.csv('iris.csv', header=True, inferSchema=True)\n",
    "raw_df.show(10)\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have already explored the dataset using [this notebook](https://www.kaggle.com/aceccon/1-iris-dataset-data-exploration/notebook) and we saw **no missing values**, no suspicious duplicates and in general this dataset is well known for being clean and ML ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|label|\n",
      "+------------+-----------+------------+-----------+-------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|  2.0|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|  2.0|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|  2.0|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|  2.0|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|  2.0|\n",
      "+------------+-----------+------------+-----------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "df = indexer.fit(raw_df).transform(raw_df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Spark ML Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  2.0|[5.1,3.5,1.4,0.2]|\n",
      "|  2.0|[4.9,3.0,1.4,0.2]|\n",
      "|  2.0|[4.7,3.2,1.3,0.2]|\n",
      "|  2.0|[4.6,3.1,1.5,0.2]|\n",
      "|  2.0|[5.0,3.6,1.4,0.2]|\n",
      "|  2.0|[5.4,3.9,1.7,0.4]|\n",
      "|  2.0|[4.6,3.4,1.4,0.3]|\n",
      "|  2.0|[5.0,3.4,1.5,0.2]|\n",
      "|  2.0|[4.4,2.9,1.4,0.2]|\n",
      "|  2.0|[4.9,3.1,1.5,0.1]|\n",
      "+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectors2features = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
    "df = vectors2features.transform(df)\n",
    "df = df.select(df['label'], df['features'])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #rows: 123\n",
      "Test #rows: 27\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=2405)\n",
    "print \"Train #rows:\", train.count()\n",
    "print \"Test #rows:\", test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+\n",
      "|prediction|label|         features|\n",
      "+----------+-----+-----------------+\n",
      "|       0.0|  0.0|[5.0,2.3,3.3,1.0]|\n",
      "|       0.0|  0.0|[5.5,2.3,4.0,1.3]|\n",
      "|       0.0|  0.0|[5.5,2.4,3.7,1.0]|\n",
      "|       0.0|  0.0|[5.5,2.4,3.8,1.1]|\n",
      "|       0.0|  0.0|[5.5,2.5,4.0,1.3]|\n",
      "|       0.0|  0.0|[5.7,2.6,3.5,1.0]|\n",
      "|       0.0|  0.0|[5.7,2.8,4.1,1.3]|\n",
      "|       0.0|  0.0|[5.9,3.0,4.2,1.5]|\n",
      "|       0.0|  0.0|[6.3,2.3,4.4,1.3]|\n",
      "|       0.0|  0.0|[6.4,2.9,4.3,1.3]|\n",
      "+----------+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy on test data is 0.925925925926\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf_model = RandomForestClassifier(numTrees=10).fit(train) # we can also use cross validation\n",
    "predictions_df = rf_model.transform(test)\n",
    "predictions_df.select('prediction', 'label', 'features').show(10)\n",
    "accuracy = MulticlassClassificationEvaluator(metricName=\"accuracy\").evaluate(predictions_df)\n",
    "print('Accuracy on test data is {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The same principals can be used for [regression models](https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#regression).\n",
    "\n",
    "* This dataset is fairly clean and ready for modeling usually we will need to use [transformations and feature engineering](https://spark.apache.org/docs/2.1.0/ml-features.html). \n",
    "\n",
    "* make sure you don't train and test yourself on the same data, generally getting suprisingly good metrics should concern you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
